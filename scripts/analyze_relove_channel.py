import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="pydantic")
warnings.filterwarnings("ignore", category=FutureWarning, module="transformers")
warnings.filterwarnings("ignore", category=FutureWarning, module="torch")

import argparse
import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
from tqdm.asyncio import tqdm

from relove_bot.config import settings
from relove_bot.services.relove_channel_analysis import (
    get_channel_messages, split_batches, analyze_batch, save_report
)
from relove_bot.services.telegram_service import get_client
from relove_bot.services.llm_service import get_llm_service

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def analyze_content_structure(messages: List[Dict[str, Any]]) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∫–∞–Ω–∞–ª–∞"""
    llm = get_llm_service()
    
    prompt = """
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∫–∞–Ω–∞–ª–∞ reLove –∏ –≤—ã–¥–µ–ª–∏:
    
    1. –û–°–ù–û–í–ù–´–ï –¢–ï–ú–´ –ò –ö–û–ù–¶–ï–ü–¶–ò–ò:
    - –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã –∏ –∏–¥–µ–∏
    - –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏ –º–µ—Ç–∞—Ñ–æ—Ä—ã
    - –¶–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã
    
    2. –ú–ï–¢–û–î–´ –ò –ü–†–ê–ö–¢–ò–ö–ò:
    - –¢–µ—Ö–Ω–∏–∫–∏ –∏ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è
    - –ü—Ä–æ—Ü–µ—Å—Å—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    - –ì—Ä—É–ø–ø–æ–≤—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏
    
    3. –¶–ï–õ–ï–í–ê–Ø –ê–£–î–ò–¢–û–†–ò–Ø:
    - –£—Ä–æ–≤–Ω–∏ —Ä–∞–∑–≤–∏—Ç–∏—è
    - –ü–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –∏ –∑–∞–ø—Ä–æ—Å—ã
    - –¢–æ—á–∫–∏ —Ä–æ—Å—Ç–∞
    
    4. –°–¢–†–£–ö–¢–£–†–ê –ü–û–¢–û–ö–û–í:
    - –≠—Ç–∞–ø—ã —Ä–∞–∑–≤–∏—Ç–∏—è
    - –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏
    - –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    
    –û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
    {
        "themes": {
            "key_themes": ["—Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º"],
            "concepts": ["–æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏"],
            "values": ["—Ü–µ–Ω–Ω–æ—Å—Ç–∏ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞"]
        },
        "practices": {
            "techniques": ["—Å–ø–∏—Å–æ–∫ —Ç–µ—Ö–Ω–∏–∫"],
            "processes": ["–ø—Ä–æ—Ü–µ—Å—Å—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏"],
            "group_practices": ["–≥—Ä—É–ø–ø–æ–≤—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏"]
        },
        "audience": {
            "development_levels": ["—É—Ä–æ–≤–Ω–∏ —Ä–∞–∑–≤–∏—Ç–∏—è"],
            "needs": ["–ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏"],
            "growth_points": ["—Ç–æ—á–∫–∏ —Ä–æ—Å—Ç–∞"]
        },
        "streams": {
            "stages": ["—ç—Ç–∞–ø—ã —Ä–∞–∑–≤–∏—Ç–∏—è"],
            "key_practices": ["–∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏"],
            "expected_results": ["–æ–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"]
        }
    }
    """
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –±–∞—Ç—á–∞–º–∏
    batches = split_batches(messages, 30)
    all_analyses = []
    
    async for batch in tqdm(batches, desc="–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞", unit="–±–∞—Ç—á"):
        analysis = await llm.analyze(prompt, json.dumps(batch, ensure_ascii=False))
        all_analyses.append(analysis)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    combined_analysis = {
        "themes": {
            "key_themes": [],
            "concepts": [],
            "values": []
        },
        "practices": {
            "techniques": [],
            "processes": [],
            "group_practices": []
        },
        "audience": {
            "development_levels": [],
            "needs": [],
            "growth_points": []
        },
        "streams": {
            "stages": [],
            "key_practices": [],
            "expected_results": []
        }
    }
    
    # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for analysis in all_analyses:
        if isinstance(analysis, dict):
            for category in combined_analysis:
                for key in combined_analysis[category]:
                    if key in analysis.get(category, {}):
                        combined_analysis[category][key].extend(analysis[category][key])
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    for category in combined_analysis:
        for key in combined_analysis[category]:
            combined_analysis[category][key] = list(set(combined_analysis[category][key]))
    
    return combined_analysis

async def analyze_core_meaning(messages: List[Dict[str, Any]]) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–º—ã—Å–ª–æ–≤ –∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π"""
    llm = get_llm_service()
    
    prompt = """
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–æ–æ–±—â–µ–Ω–∏—è –∫–∞–Ω–∞–ª–∞ reLove –∏ –≤—ã–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–º—ã—Å–ª—ã –∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏:
    
    1. –ö–õ–Æ–ß–ï–í–´–ï –°–ú–´–°–õ–´:
    - –û—Å–Ω–æ–≤–Ω—ã–µ –∏–¥–µ–∏ –∏ –ø–æ—Å—ã–ª—ã
    - –ì–ª—É–±–∏–Ω–Ω—ã–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏
    - –ú–µ—Ç–∞—Ñ–æ—Ä—ã –∏ –æ–±—Ä–∞–∑—ã
    
    2. –ö–û–ù–¶–ï–ü–¶–ò–ò –†–ê–ó–í–ò–¢–ò–Ø:
    - –≠—Ç–∞–ø—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    - –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏
    - –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    
    3. –ú–ï–¢–û–î–û–õ–û–ì–ò–Ø:
    - –ü–æ–¥—Ö–æ–¥—ã –∫ —Ä–∞–±–æ—Ç–µ
    - –¢–µ—Ö–Ω–∏–∫–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
    - –ü—Ä–æ—Ü–µ—Å—Å—ã –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    
    4. –¶–ï–ù–ù–û–°–¢–ò –ò –ü–†–ò–ù–¶–ò–ü–´:
    - –ë–∞–∑–æ–≤—ã–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏
    - –≠—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã
    - –ü—Ä–∞–≤–∏–ª–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
    
    –û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
    {
        "core_meanings": {
            "key_ideas": ["–æ—Å–Ω–æ–≤–Ω—ã–µ –∏–¥–µ–∏"],
            "deep_values": ["–≥–ª—É–±–∏–Ω–Ω—ã–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏"],
            "metaphors": ["–º–µ—Ç–∞—Ñ–æ—Ä—ã –∏ –æ–±—Ä–∞–∑—ã"]
        },
        "development": {
            "transformation_stages": ["—ç—Ç–∞–ø—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏"],
            "key_practices": ["–∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏"],
            "expected_outcomes": ["–æ–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"]
        },
        "methodology": {
            "approaches": ["–ø–æ–¥—Ö–æ–¥—ã –∫ —Ä–∞–±–æ—Ç–µ"],
            "techniques": ["—Ç–µ—Ö–Ω–∏–∫–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã"],
            "processes": ["–ø—Ä–æ—Ü–µ—Å—Å—ã –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"]
        },
        "values": {
            "core_values": ["–±–∞–∑–æ–≤—ã–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏"],
            "principles": ["—ç—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã"],
            "interaction_rules": ["–ø—Ä–∞–≤–∏–ª–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è"]
        }
    }
    """
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –±–∞—Ç—á–∞–º–∏
    batches = split_batches(messages, 30)
    all_analyses = []
    
    async for batch in tqdm(batches, desc="–ê–Ω–∞–ª–∏–∑ —Å–º—ã—Å–ª–æ–≤", unit="–±–∞—Ç—á"):
        analysis = await llm.analyze(prompt, json.dumps(batch, ensure_ascii=False))
        all_analyses.append(analysis)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    combined_analysis = {
        "core_meanings": {
            "key_ideas": [],
            "deep_values": [],
            "metaphors": []
        },
        "development": {
            "transformation_stages": [],
            "key_practices": [],
            "expected_outcomes": []
        },
        "methodology": {
            "approaches": [],
            "techniques": [],
            "processes": []
        },
        "values": {
            "core_values": [],
            "principles": [],
            "interaction_rules": []
        }
    }
    
    # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for analysis in all_analyses:
        if isinstance(analysis, dict):
            for category in combined_analysis:
                for key in combined_analysis[category]:
                    if key in analysis.get(category, {}):
                        combined_analysis[category][key].extend(analysis[category][key])
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    for category in combined_analysis:
        for key in combined_analysis[category]:
            combined_analysis[category][key] = list(set(combined_analysis[category][key]))
    
    return combined_analysis

def generate_md_report(structure_analysis: Dict[str, Any], messages_analysis: Dict[str, Any], 
                      core_meaning: Dict[str, Any], timestamp: str) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown"""
    report = [
        "# üìä –ê–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–∞ reLove\n",
        f"## üìÖ –î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {datetime.now().strftime('%d.%m.%Y %H:%M')}\n",
        f"## üìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞",
        f"- –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {messages_analysis['total_messages']}",
        f"- –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –±–∞—Ç—á–µ–π: {messages_analysis['batches']}\n",
        
        "## üéØ –ö–ª—é—á–µ–≤—ã–µ —Å–º—ã—Å–ª—ã –∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏\n"
    ]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–º—ã—Å–ª–æ–≤
    for category, items in core_meaning.items():
        report.append(f"### {category.replace('_', ' ').title()}")
        for key, values in items.items():
            if values:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏
                report.append(f"\n#### {key.replace('_', ' ').title()}")
                for value in values:
                    report.append(f"- {value}")
        report.append("")
    
    report.append("## üéØ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞\n")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    for category, items in structure_analysis.items():
        report.append(f"### {category.title()}")
        for key, values in items.items():
            if values:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏
                report.append(f"\n#### {key.replace('_', ' ').title()}")
                for value in values:
                    report.append(f"- {value}")
        report.append("")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π
    report.append("## üìù –ê–Ω–∞–ª–∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π\n")
    for i, batch in enumerate(messages_analysis['analyses'], 1):
        report.append(f"### –ë–∞—Ç—á {i}")
        if isinstance(batch.get('analysis'), dict):
            for key, value in batch['analysis'].items():
                report.append(f"\n#### {key.replace('_', ' ').title()}")
                if isinstance(value, list):
                    for item in value:
                        report.append(f"- {item}")
                else:
                    report.append(f"- {value}")
        else:
            report.append(batch.get('analysis', '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö'))
        report.append("")
    
    return "\n".join(report)

async def main():
    parser = argparse.ArgumentParser(
        description="–ê–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–∞ reLove —á–µ—Ä–µ–∑ LLM.\n\n"
                    "–ë–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –≤—Å–µ –ø–æ—Å—Ç—ã (–¥–æ 10 000).\n"
                    "--last N ‚Äî —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N, --batch-size N ‚Äî —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--last', type=int, help='–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –ø–æ—Å—Ç–æ–≤')
    parser.add_argument('--batch-size', type=int, default=30, help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä–µ–∑ LLM (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 30)')
    parser.add_argument('--channel', type=str, default='reloveinfo', help='–Æ–∑–µ—Ä–Ω–µ–π–º –∫–∞–Ω–∞–ª–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é reloveinfo)')
    args = parser.parse_args()

    # –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω --last, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –ø–æ—Å—Ç—ã
    if not args.last:
        logger.info("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ —É–∫–∞–∑–∞–Ω—ã ‚Äî –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –ø–æ—Å—Ç—ã (–¥–æ 10 000).")

    client = await get_client()
    try:
        if args.last:
            messages = await get_channel_messages(client, args.channel, limit=args.last)
        else:
            messages = await get_channel_messages(client, args.channel, limit=None)
            
        if not messages:
            print("–ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
            return
            
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        logger.info("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        structure_analysis = await analyze_content_structure(messages)
        
        # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–º—ã—Å–ª–æ–≤
        logger.info("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–º—ã—Å–ª—ã...")
        core_meaning = await analyze_core_meaning(messages)
        
        # –ê–Ω–∞–ª–∏–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        batches = split_batches(messages, args.batch_size)
        all_analyses = []
        
        async for batch in tqdm(batches, desc="–ê–Ω–∞–ª–∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π", unit="–±–∞—Ç—á"):
            analysis = await analyze_batch(batch, len(all_analyses) + 1)
            all_analyses.append(analysis)
            
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_dir = Path("reports")
        report_dir.mkdir(exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        structure_file = report_dir / f"relove_structure_analysis_{timestamp}.json"
        with open(structure_file, "w", encoding="utf-8") as f:
            json.dump(structure_analysis, f, ensure_ascii=False, indent=2)
            
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–º—ã—Å–ª–æ–≤
        meaning_file = report_dir / f"relove_core_meaning_{timestamp}.json"
        with open(meaning_file, "w", encoding="utf-8") as f:
            json.dump(core_meaning, f, ensure_ascii=False, indent=2)
            
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π
        messages_file = report_dir / f"relove_messages_analysis_{timestamp}.json"
        with open(messages_file, "w", encoding="utf-8") as f:
            json.dump({
                "total_messages": len(messages),
                "batches": len(batches),
                "analyses": all_analyses
            }, f, ensure_ascii=False, indent=2)
            
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º MD-–æ—Ç—á–µ—Ç
        md_report = generate_md_report(structure_analysis, {
            "total_messages": len(messages),
            "batches": len(batches),
            "analyses": all_analyses
        }, core_meaning, timestamp)
        
        md_file = report_dir / f"relove_analysis_{timestamp}.md"
        with open(md_file, "w", encoding="utf-8") as f:
            f.write(md_report)
            
        print(f"\n–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {len(messages)}. –ë–∞—Ç—á–µ–π: {len(batches)}.")
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:")
        print(f"- –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {structure_file}")
        print(f"- –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–º—ã—Å–ª–æ–≤: {meaning_file}")
        print(f"- –ê–Ω–∞–ª–∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π: {messages_file}")
        print(f"- Markdown –æ—Ç—á–µ—Ç: {md_file}")
        
    finally:
        await client.disconnect()

if __name__ == "__main__":
    asyncio.run(main()) 