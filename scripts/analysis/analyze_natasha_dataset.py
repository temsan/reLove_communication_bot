#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–∏–∞–ª–æ–≥–æ–≤ –ù–∞—Ç–∞—à–∏ –¥–ª—è fine-tuning.

–ó–∞–¥–∞—á–∏:
1. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏–Ω–µ —Å–æ–æ–±—â–µ–Ω–∏–π
2. –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ –∫–∞–Ω–∞–ª–∞–º
3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ QA –ø–∞—Ä
4. –í—ã—è–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
5. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
"""
import json
import csv
import sys
from pathlib import Path
from typing import Dict, List, Any
from collections import defaultdict
import statistics

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DatasetAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞."""
    
    def __init__(self, json_path: str):
        self.json_path = json_path
        self.data = None
        self.dialogs = []
        self.qa_pairs = []
        self.statistics = {}
        
        self._load_data()
    
    def _load_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON."""
        logger.info(f"Loading dataset from {self.json_path}")
        
        with open(self.json_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        self.dialogs = self.data.get('dialogs', [])
        logger.info(f"Loaded {len(self.dialogs)} dialogs")
    
    def analyze_message_lengths(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–ª–∏–Ω—É —Å–æ–æ–±—â–µ–Ω–∏–π."""
        logger.info("\n" + "="*70)
        logger.info("MESSAGE LENGTH ANALYSIS")
        logger.info("="*70)
        
        user_lengths = []
        natasha_lengths = []
        
        for dialog in self.dialogs:
            user_msg = dialog.get('user_message', '')
            natasha_msg = dialog.get('natasha_response', '')
            
            if user_msg:
                user_lengths.append(len(user_msg))
            if natasha_msg:
                natasha_lengths.append(len(natasha_msg))
        
        stats = {
            'user_messages': {
                'count': len(user_lengths),
                'min': min(user_lengths) if user_lengths else 0,
                'max': max(user_lengths) if user_lengths else 0,
                'mean': statistics.mean(user_lengths) if user_lengths else 0,
                'median': statistics.median(user_lengths) if user_lengths else 0,
                'stdev': statistics.stdev(user_lengths) if len(user_lengths) > 1 else 0,
            },
            'natasha_messages': {
                'count': len(natasha_lengths),
                'min': min(natasha_lengths) if natasha_lengths else 0,
                'max': max(natasha_lengths) if natasha_lengths else 0,
                'mean': statistics.mean(natasha_lengths) if natasha_lengths else 0,
                'median': statistics.median(natasha_lengths) if natasha_lengths else 0,
                'stdev': statistics.stdev(natasha_lengths) if len(natasha_lengths) > 1 else 0,
            }
        }
        
        logger.info("\nUser Messages:")
        logger.info(f"  Count: {stats['user_messages']['count']}")
        logger.info(f"  Min: {stats['user_messages']['min']} chars")
        logger.info(f"  Max: {stats['user_messages']['max']} chars")
        logger.info(f"  Mean: {stats['user_messages']['mean']:.1f} chars")
        logger.info(f"  Median: {stats['user_messages']['median']} chars")
        logger.info(f"  Stdev: {stats['user_messages']['stdev']:.1f}")
        
        logger.info("\nNatasha Messages:")
        logger.info(f"  Count: {stats['natasha_messages']['count']}")
        logger.info(f"  Min: {stats['natasha_messages']['min']} chars")
        logger.info(f"  Max: {stats['natasha_messages']['max']} chars")
        logger.info(f"  Mean: {stats['natasha_messages']['mean']:.1f} chars")
        logger.info(f"  Median: {stats['natasha_messages']['median']} chars")
        logger.info(f"  Stdev: {stats['natasha_messages']['stdev']:.1f}")
        
        self.statistics['message_lengths'] = stats
        return stats
    
    def analyze_channel_distribution(self) -> Dict[str, int]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞–Ω–∞–ª–∞–º."""
        logger.info("\n" + "="*70)
        logger.info("CHANNEL DISTRIBUTION")
        logger.info("="*70)
        
        channel_counts = defaultdict(int)
        
        for dialog in self.dialogs:
            channel = dialog.get('channel', 'Unknown')
            channel_counts[channel] += 1
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É
        sorted_channels = sorted(
            channel_counts.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        logger.info(f"\nTotal channels: {len(sorted_channels)}")
        logger.info("\nTop channels:")
        
        for i, (channel, count) in enumerate(sorted_channels[:10], 1):
            percentage = (count / len(self.dialogs)) * 100
            logger.info(f"  {i}. {channel}: {count} dialogs ({percentage:.1f}%)")
        
        if len(sorted_channels) > 10:
            logger.info(f"  ... and {len(sorted_channels) - 10} more channels")
        
        self.statistics['channel_distribution'] = dict(sorted_channels)
        return dict(sorted_channels)
    
    def analyze_dialog_types(self) -> Dict[str, int]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–∏–ø—ã –¥–∏–∞–ª–æ–≥–æ–≤."""
        logger.info("\n" + "="*70)
        logger.info("DIALOG TYPES")
        logger.info("="*70)
        
        type_counts = defaultdict(int)
        
        for dialog in self.dialogs:
            dialog_type = dialog.get('type', 'unknown')
            type_counts[dialog_type] += 1
        
        logger.info("\nDialog types:")
        for dialog_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(self.dialogs)) * 100
            logger.info(f"  {dialog_type}: {count} ({percentage:.1f}%)")
        
        self.statistics['dialog_types'] = dict(type_counts)
        return dict(type_counts)
    
    def analyze_quality_metrics(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞."""
        logger.info("\n" + "="*70)
        logger.info("QUALITY METRICS")
        logger.info("="*70)
        
        metrics = {
            'total_dialogs': len(self.dialogs),
            'dialogs_with_context': 0,
            'dialogs_with_reply_chain': 0,
            'avg_context_length': 0,
            'quality_issues': []
        }
        
        context_lengths = []
        
        for dialog in self.dialogs:
            context = dialog.get('context', [])
            if context:
                metrics['dialogs_with_context'] += 1
                context_lengths.append(len(context))
            
            if dialog.get('type') == 'reply_chain':
                metrics['dialogs_with_reply_chain'] += 1
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            user_msg = dialog.get('user_message', '')
            natasha_msg = dialog.get('natasha_response', '')
            
            # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            if len(user_msg) < 30:
                metrics['quality_issues'].append({
                    'type': 'short_user_message',
                    'channel': dialog.get('channel'),
                    'length': len(user_msg)
                })
            
            if len(natasha_msg) < 20:
                metrics['quality_issues'].append({
                    'type': 'short_natasha_message',
                    'channel': dialog.get('channel'),
                    'length': len(natasha_msg)
                })
        
        if context_lengths:
            metrics['avg_context_length'] = statistics.mean(context_lengths)
        
        logger.info(f"\nTotal dialogs: {metrics['total_dialogs']}")
        logger.info(f"Dialogs with context: {metrics['dialogs_with_context']} ({(metrics['dialogs_with_context']/metrics['total_dialogs']*100):.1f}%)")
        logger.info(f"Dialogs with reply chain: {metrics['dialogs_with_reply_chain']} ({(metrics['dialogs_with_reply_chain']/metrics['total_dialogs']*100):.1f}%)")
        logger.info(f"Average context length: {metrics['avg_context_length']:.1f} messages")
        
        # –ö–∞—á–µ—Å—Ç–≤–æ
        logger.info(f"\nQuality issues found: {len(metrics['quality_issues'])}")
        
        issue_types = defaultdict(int)
        for issue in metrics['quality_issues']:
            issue_types[issue['type']] += 1
        
        for issue_type, count in issue_types.items():
            logger.info(f"  {issue_type}: {count}")
        
        self.statistics['quality_metrics'] = metrics
        return metrics
    
    def analyze_content_themes(self) -> Dict[str, int]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–º—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
        logger.info("\n" + "="*70)
        logger.info("CONTENT THEMES")
        logger.info("="*70)
        
        themes = {
            'spiritual_growth': 0,
            'past_lives': 0,
            'rituals': 0,
            'relationships': 0,
            'business': 0,
            'personal_transformation': 0,
            'energy_work': 0,
            'questions': 0
        }
        
        keywords = {
            'spiritual_growth': ['–ø—É—Ç—å', '—Ä–∞–∑–≤–∏—Ç–∏–µ', '—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è', '–æ—Å–æ–∑–Ω–∞–Ω–∏–µ', '–ø—Ä–æ–∑—Ä–µ–Ω–∏–µ'],
            'past_lives': ['–ø—Ä–æ—à–ª–∞—è –∂–∏–∑–Ω—å', '–∏–Ω–∫–∞—Ä–Ω–∞—Ü–∏—è', '–≤–æ–ø–ª–æ—â–µ–Ω–∏–µ', '–ü–ñ', '–∂–∏–∑–Ω—å'],
            'rituals': ['—Ä–∏—Ç—É–∞–ª', '–º–µ–¥–∏—Ç–∞—Ü–∏—è', '–ø—Ä–∞–∫—Ç–∏–∫–∞', '–º–∏—Å—Ç–µ—Ä–∏—è'],
            'relationships': ['–æ—Ç–Ω–æ—à–µ–Ω–∏–µ', '–º—É–∂—á–∏–Ω–∞', '–∂–µ–Ω—â–∏–Ω–∞', '–ª—é–±–æ–≤—å', '–ø–∞—Ä—Ç–Ω–µ—Ä'],
            'business': ['–±–∏–∑–Ω–µ—Å', '–ø—Ä–æ–µ–∫—Ç', '–ø—Ä–æ–¥–∞–∂–∏', '–º–∞—Ä–∫–µ—Ç–∏–Ω–≥', '—Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ'],
            'personal_transformation': ['–∏–∑–º–µ–Ω–µ–Ω–∏–µ', '—Å—Ç–∞–ª', '—Å—Ç–∞–ª–∞', '–∏–∑–º–µ–Ω–∏–ª–∞—Å—å', '–ø—Ä–µ–æ–¥–æ–ª–µ–ª'],
            'energy_work': ['—ç–Ω–µ—Ä–≥–∏—è', '–≤–∏–±—Ä–∞—Ü–∏—è', '—Ü–µ–Ω—Ç—Ä', '–ø–æ—Ç–æ–∫', '–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ'],
            'questions': ['?']
        }
        
        for dialog in self.dialogs:
            natasha_msg = dialog.get('natasha_response', '').lower()
            user_msg = dialog.get('user_message', '').lower()
            combined = natasha_msg + ' ' + user_msg
            
            for theme, keywords_list in keywords.items():
                if any(kw in combined for kw in keywords_list):
                    themes[theme] += 1
        
        logger.info("\nContent themes:")
        for theme, count in sorted(themes.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(self.dialogs)) * 100
            logger.info(f"  {theme}: {count} ({percentage:.1f}%)")
        
        self.statistics['content_themes'] = themes
        return themes
    
    def generate_recommendations(self) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
        logger.info("\n" + "="*70)
        logger.info("RECOMMENDATIONS")
        logger.info("="*70)
        
        recommendations = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞
        if len(self.dialogs) < 100:
            recommendations.append("‚ö†Ô∏è  –î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –º–µ–Ω–µ–µ 100 –¥–∏–∞–ª–æ–≥–æ–≤. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–æ–±—Ä–∞—Ç—å –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ª—É—á—à–µ–≥–æ fine-tuning.")
        elif len(self.dialogs) < 500:
            recommendations.append("‚úì –î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ fine-tuning (100-500 –ø—Ä–∏–º–µ—Ä–æ–≤).")
        else:
            recommendations.append("‚úì –î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ fine-tuning (500+ –ø—Ä–∏–º–µ—Ä–æ–≤).")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞–Ω–∞–ª–∞–º
        channel_dist = self.statistics.get('channel_distribution', {})
        if channel_dist:
            max_channel_count = max(channel_dist.values())
            max_percentage = (max_channel_count / len(self.dialogs)) * 100
            
            if max_percentage > 50:
                recommendations.append(f"‚ö†Ô∏è  –û–¥–∏–Ω –∫–∞–Ω–∞–ª —Å–æ–¥–µ—Ä–∂–∏—Ç {max_percentage:.1f}% –¥–∞–Ω–Ω—ã—Ö. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤.")
            else:
                recommendations.append("‚úì –î–∞–Ω–Ω—ã–µ —Ö–æ—Ä–æ—à–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –ø–æ –∫–∞–Ω–∞–ª–∞–º.")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É —Å–æ–æ–±—â–µ–Ω–∏–π
        msg_lengths = self.statistics.get('message_lengths', {})
        if msg_lengths:
            natasha_mean = msg_lengths.get('natasha_messages', {}).get('mean', 0)
            
            if natasha_mean < 50:
                recommendations.append("‚ö†Ô∏è  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–æ–≤ –ù–∞—Ç–∞—à–∏ –º–µ–Ω–µ–µ 50 —Å–∏–º–≤–æ–ª–æ–≤. –≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –∫–æ—Ä–æ—Ç–∫–∏–º –æ—Ç–≤–µ—Ç–∞–º –º–æ–¥–µ–ª–∏.")
            elif natasha_mean > 500:
                recommendations.append("‚úì –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–æ–≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞ –¥–ª—è fine-tuning.")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        quality = self.statistics.get('quality_metrics', {})
        if quality:
            issues = quality.get('quality_issues', [])
            if len(issues) > len(self.dialogs) * 0.1:
                recommendations.append(f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {len(issues)} –ø—Ä–æ–±–ª–µ–º —Å –∫–∞—á–µ—Å—Ç–≤–æ–º ({len(issues)/len(self.dialogs)*100:.1f}%). –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞–Ω–Ω—ã–µ.")
            else:
                recommendations.append("‚úì –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö —Ö–æ—Ä–æ—à–µ–µ.")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ fine-tuning
        recommendations.append("\nüìö –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ fine-tuning:")
        recommendations.append("  1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ JSONL —Ñ–∞–π–ª (natasha_finetuning_*.jsonl)")
        recommendations.append("  2. –ù–∞—á–Ω–∏—Ç–µ —Å 3 —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è")
        recommendations.append("  3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ learning_rate_multiplier = 0.1")
        recommendations.append("  4. –¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤")
        recommendations.append("  5. –ö–æ–º–±–∏–Ω–∏—Ä—É–π—Ç–µ —Å –¥—Ä—É–≥–∏–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç–∏")
        
        for rec in recommendations:
            logger.info(rec)
        
        return recommendations
    
    def save_analysis_report(self, output_path: str = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á–µ—Ç –∞–Ω–∞–ª–∏–∑–∞."""
        if not output_path:
            output_path = f"data/dataset_analysis_{Path(self.json_path).stem}.json"
        
        report = {
            'dataset_file': self.json_path,
            'statistics': self.statistics
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\n‚úÖ Analysis report saved to: {output_path}")
        return output_path
    
    def run_full_analysis(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑."""
        logger.info("\n" + "="*70)
        logger.info("NATASHA DATASET ANALYSIS")
        logger.info("="*70)
        
        self.analyze_message_lengths()
        self.analyze_channel_distribution()
        self.analyze_dialog_types()
        self.analyze_quality_metrics()
        self.analyze_content_themes()
        self.generate_recommendations()
        
        self.save_analysis_report()
        
        logger.info("\n" + "="*70)
        logger.info("ANALYSIS COMPLETE")
        logger.info("="*70 + "\n")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Analyze Natasha's dialogs dataset"
    )
    parser.add_argument(
        '--input',
        type=str,
        default='data/natasha_dialogs_dataset_20251125_153356.json',
        help='Input JSON file with dialogs'
    )
    
    args = parser.parse_args()
    
    if not Path(args.input).exists():
        logger.error(f"File not found: {args.input}")
        return
    
    analyzer = DatasetAnalyzer(args.input)
    analyzer.run_full_analysis()


if __name__ == "__main__":
    main()
