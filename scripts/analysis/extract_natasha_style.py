"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç–∏–ª—è –ù–∞—Ç–∞—à–∏ –í–æ–ª–∫–æ—à –∏–∑ –∫–∞–Ω–∞–ª–æ–≤ –∏ —á–∞—Ç–æ–≤.
1. –ò–∑ –ø–æ—Å—Ç–æ–≤ –≤ broadcast –∫–∞–Ω–∞–ª–∞—Ö - –≤—ã–∂–∏–º–∫–∞ —è–∑—ã–∫–∞
2. –ò–∑ —á–∞—Ç–æ–≤ - –ø—Ä–æ–≤–æ–∫–∞—Ç–∏–≤–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏
"""
import asyncio
import sys
from pathlib import Path
from typing import List, Dict
import json
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from telethon import TelegramClient
from telethon.tl.types import Message
from relove_bot.config import settings
from relove_bot.rag.llm import LLM
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/extract_natasha_style.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


# –ö–∞–Ω–∞–ª—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å—Ç–æ–≤
BROADCAST_CHANNELS = [
    'reloverituals',  # reLove rituals
    # reLoveüåÄ| –ü—É—Ç—å –ì–µ—Ä–æ—è - –Ω–µ—Ç username
    # reLove. –ë–æ–ª—å—à–∞—è –ò–≥—Ä–∞ —Å –ù–∞—Ç–∞—à–µ–π - –Ω–µ—Ç username
    # reLove people - –Ω–µ—Ç username
    # –ü—Ä–æ—à–ª—ã–µ –ñ–∏–∑–Ω–∏ reLove - –Ω–µ—Ç username
]

# –ß–∞—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–∞–ª–æ–≥–æ–≤
DISCUSSION_CHATS = [
    # –ß–ê–¢ RELOVE - –Ω–µ—Ç username
    # reLove ‚Äì –ë–æ–ª—å—à–∞—è –ò–≥—Ä–∞ - –Ω–µ—Ç username
    # reLove people Chat - –Ω–µ—Ç username
    # reLoveüåÄ| –ü—É—Ç—å –ì–µ—Ä–æ—è Chat - –Ω–µ—Ç username
    # –ß–ê–¢ –ü—Ä–æ—à–ª—ã–µ –∂–∏–∑–Ω–∏ reLove - –Ω–µ—Ç username
]


class NatashaStyleExtractor:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∏–ª—å –ù–∞—Ç–∞—à–∏ –í–æ–ª–∫–æ—à –∏–∑ –∫–∞–Ω–∞–ª–æ–≤"""
    
    def __init__(self):
        self.client = TelegramClient(
            settings.tg_session,
            settings.tg_api_id,
            settings.tg_api_hash.get_secret_value()
        )
        self.llm = LLM()
        self.natasha_posts = []
        self.natasha_dialogs = []
    
    async def find_channels_by_name(self, keywords: List[str]) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–∞–Ω–∞–ª—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏"""
        logger.info(f"Searching for channels with keywords: {keywords}")
        found = []
        
        async for dialog in self.client.iter_dialogs():
            name_lower = dialog.name.lower()
            
            for keyword in keywords:
                if keyword.lower() in name_lower:
                    channel_info = {
                        'id': dialog.id,
                        'name': dialog.name,
                        'username': getattr(dialog.entity, 'username', None),
                        'entity': dialog.entity
                    }
                    found.append(channel_info)
                    logger.info(f"   Found: {channel_info['name']}")
                    break
        
        return found
    
    async def extract_posts_from_channel(
        self, 
        channel_entity, 
        channel_name: str,
        limit: int = 100
    ):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ—Å—Ç—ã –∏–∑ –∫–∞–Ω–∞–ª–∞"""
        logger.info(f"\n{'='*70}")
        logger.info(f"Extracting posts from: {channel_name}")
        logger.info(f"{'='*70}")
        
        posts = []
        count = 0
        
        try:
            async for message in self.client.iter_messages(channel_entity, limit=limit):
                if message.text and len(message.text) > 50:  # –¢–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ—Å—Ç—ã
                    post_data = {
                        'channel': channel_name,
                        'date': message.date.isoformat(),
                        'text': message.text,
                        'views': message.views or 0,
                        'forwards': message.forwards or 0
                    }
                    posts.append(post_data)
                    count += 1
                    
                    if count <= 3:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                        logger.info(f"\nPost {count}:")
                        logger.info(f"Date: {message.date}")
                        logger.info(f"Text preview: {message.text[:200]}...")
            
            logger.info(f"\n‚úÖ Extracted {len(posts)} posts from {channel_name}")
            self.natasha_posts.extend(posts)
            
        except Exception as e:
            logger.error(f"‚ùå Error extracting from {channel_name}: {e}")
    
    async def extract_dialogs_from_chat(
        self,
        chat_entity,
        chat_name: str,
        limit: int = 500
    ):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∏–∞–ª–æ–≥–∏ –∏–∑ —á–∞—Ç–∞, —Ñ–æ–∫—É—Å–∏—Ä—É—è—Å—å –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö –ù–∞—Ç–∞—à–∏"""
        logger.info(f"\n{'='*70}")
        logger.info(f"Extracting dialogs from: {chat_name}")
        logger.info(f"{'='*70}")
        
        # ID –ù–∞—Ç–∞—à–∏ –í–æ–ª–∫–æ—à (–µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–µ–Ω)
        NATASHA_ID = 496684653  # @NatashaVolkosh
        
        dialogs = []
        natasha_messages = []
        
        try:
            messages_list = []
            async for message in self.client.iter_messages(chat_entity, limit=limit):
                messages_list.append(message)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (—Å—Ç–∞—Ä—ã–µ -> –Ω–æ–≤—ã–µ)
            messages_list.sort(key=lambda m: m.date)
            
            # –ò—â–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –ù–∞—Ç–∞—à–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –Ω–∏—Ö
            for i, message in enumerate(messages_list):
                if message.sender_id == NATASHA_ID and message.text:
                    # –ë–µ—Ä–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç: 2 —Å–æ–æ–±—â–µ–Ω–∏—è –¥–æ –∏ 2 –ø–æ—Å–ª–µ
                    context_start = max(0, i - 2)
                    context_end = min(len(messages_list), i + 3)
                    
                    dialog_context = []
                    for j in range(context_start, context_end):
                        msg = messages_list[j]
                        if msg.text:
                            dialog_context.append({
                                'sender_id': msg.sender_id,
                                'sender_name': getattr(msg.sender, 'first_name', 'Unknown') if msg.sender else 'Unknown',
                                'is_natasha': msg.sender_id == NATASHA_ID,
                                'text': msg.text,
                                'date': msg.date.isoformat()
                            })
                    
                    if len(dialog_context) > 1:  # –ï—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
                        dialog_data = {
                            'chat': chat_name,
                            'date': message.date.isoformat(),
                            'context': dialog_context
                        }
                        dialogs.append(dialog_data)
                        natasha_messages.append(message.text)
            
            logger.info(f"\n‚úÖ Found {len(natasha_messages)} messages from Natasha")
            logger.info(f"‚úÖ Extracted {len(dialogs)} dialog contexts")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
            if natasha_messages:
                logger.info(f"\nExample Natasha messages:")
                for i, msg in enumerate(natasha_messages[:3], 1):
                    logger.info(f"\n{i}. {msg[:200]}...")
            
            self.natasha_dialogs.extend(dialogs)
            
        except Exception as e:
            logger.error(f"‚ùå Error extracting from {chat_name}: {e}")
    
    async def analyze_language_style(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç–∏–ª—å —è–∑—ã–∫–∞ –ù–∞—Ç–∞—à–∏ —á–µ—Ä–µ–∑ LLM"""
        logger.info(f"\n{'='*70}")
        logger.info("ANALYZING NATASHA'S LANGUAGE STYLE")
        logger.info(f"{'='*70}")
        
        if not self.natasha_posts:
            logger.warning("No posts to analyze!")
            return {}
        
        # –ë–µ—Ä–µ–º –≤—ã–±–æ—Ä–∫—É –ø–æ—Å—Ç–æ–≤
        sample_posts = self.natasha_posts[:50]
        posts_text = "\n\n---\n\n".join([p['text'] for p in sample_posts])
        
        analysis_prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å—Ç–∏–ª—å –ø–∏—Å—å–º–∞ –ù–∞—Ç–∞—à–∏ –í–æ–ª–∫–æ—à –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—ë –ø–æ—Å—Ç–æ–≤.

–ü–û–°–¢–´ –ù–ê–¢–ê–®–ò:
{posts_text}

–í—ã–¥–µ–ª–∏:
1. –ö–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è (10-15 –ø—Ä–∏–º–µ—Ä–æ–≤)
2. –°—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ (—Ç–æ–Ω, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –æ–±—Ä–∞—â–µ–Ω–∏–µ)
3. –ü—Ä–æ–≤–æ–∫–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–µ–º—ã
4. –ú–µ—Ç–∞—Ñ–æ—Ä—ã –∏ –æ–±—Ä–∞–∑—ã
5. –¢–∏–ø–∏—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∫ –∞—É–¥–∏—Ç–æ—Ä–∏–∏
6. –ü—Ä–∏–∑—ã–≤—ã –∫ –¥–µ–π—Å—Ç–≤–∏—é

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: JSON
{{
    "key_phrases": ["—Ñ—Ä–∞–∑–∞ 1", "—Ñ—Ä–∞–∑–∞ 2", ...],
    "style_features": ["–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å 1", ...],
    "provocative_techniques": ["–ø—Ä–∏–µ–º 1", ...],
    "metaphors": ["–º–µ—Ç–∞—Ñ–æ—Ä–∞ 1", ...],
    "typical_questions": ["–≤–æ–ø—Ä–æ—Å 1", ...],
    "calls_to_action": ["–ø—Ä–∏–∑—ã–≤ 1", ...]
}}
"""
        
        try:
            logger.info("Analyzing with LLM...")
            response = await self.llm.generate_rag_answer("", analysis_prompt)
            
            # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
            try:
                # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
                import re
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    analysis = json.loads(json_match.group())
                else:
                    analysis = {'raw_response': response}
            except:
                analysis = {'raw_response': response}
            
            logger.info("‚úÖ Analysis complete")
            return analysis
            
        except Exception as e:
            logger.error(f"‚ùå Error analyzing: {e}")
            return {}
    
    async def analyze_dialog_patterns(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø—Ä–æ–≤–æ–∫–∞—Ç–∏–≤–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤"""
        logger.info(f"\n{'='*70}")
        logger.info("ANALYZING DIALOG PATTERNS")
        logger.info(f"{'='*70}")
        
        if not self.natasha_dialogs:
            logger.warning("No dialogs to analyze!")
            return {}
        
        # –ë–µ—Ä–µ–º –≤—ã–±–æ—Ä–∫—É –¥–∏–∞–ª–æ–≥–æ–≤
        sample_dialogs = self.natasha_dialogs[:30]
        
        dialogs_text = ""
        for i, dialog in enumerate(sample_dialogs, 1):
            dialogs_text += f"\n\n–î–ò–ê–õ–û–ì {i}:\n"
            for msg in dialog['context']:
                role = "–ù–ê–¢–ê–®–ê" if msg['is_natasha'] else msg['sender_name']
                dialogs_text += f"{role}: {msg['text']}\n"
        
        analysis_prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–æ–≤–æ–∫–∞—Ç–∏–≤–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏ –ù–∞—Ç–∞—à–∏ –í–æ–ª–∫–æ—à.

–î–ò–ê–õ–û–ì–ò:
{dialogs_text}

–í—ã–¥–µ–ª–∏:
1. –ü—Ä–æ–≤–æ–∫–∞—Ç–∏–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã (10-15 –ø—Ä–∏–º–µ—Ä–æ–≤)
2. –¢–µ—Ö–Ω–∏–∫–∏ –≤—Å–∫—Ä—ã—Ç–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
3. –°–ø–æ—Å–æ–±—ã —Ä–∞–±–æ—Ç—ã —Å —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ–º
4. –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏ –æ—Ç–∑–µ—Ä–∫–∞–ª–∏–≤–∞–Ω–∏—è
5. –ü—Ä—è–º—ã–µ –∫–æ–Ω—Ñ—Ä–æ–Ω—Ç–∞—Ü–∏–∏
6. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–µ —Ñ—Ä–∞–∑—ã

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: JSON
{{
    "provocative_questions": ["–≤–æ–ø—Ä–æ—Å 1", ...],
    "pattern_breaking": ["—Ç–µ—Ö–Ω–∏–∫–∞ 1", ...],
    "resistance_work": ["—Å–ø–æ—Å–æ–± 1", ...],
    "reframing": ["–ø—Ä–∏–º–µ—Ä 1", ...],
    "confrontations": ["–ø—Ä–∏–º–µ—Ä 1", ...],
    "support_phrases": ["—Ñ—Ä–∞–∑–∞ 1", ...]
}}
"""
        
        try:
            logger.info("Analyzing with LLM...")
            response = await self.llm.generate_rag_answer("", analysis_prompt)
            
            # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
            try:
                import re
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    analysis = json.loads(json_match.group())
                else:
                    analysis = {'raw_response': response}
            except:
                analysis = {'raw_response': response}
            
            logger.info("‚úÖ Analysis complete")
            return analysis
            
        except Exception as e:
            logger.error(f"‚ùå Error analyzing: {e}")
            return {}
    
    async def save_results(self, language_analysis: Dict, dialog_analysis: Dict):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
        raw_data = {
            'timestamp': timestamp,
            'posts_count': len(self.natasha_posts),
            'dialogs_count': len(self.natasha_dialogs),
            'posts': self.natasha_posts,
            'dialogs': self.natasha_dialogs
        }
        
        raw_file = f"data/natasha_raw_data_{timestamp}.json"
        Path("data").mkdir(exist_ok=True)
        
        with open(raw_file, 'w', encoding='utf-8') as f:
            json.dump(raw_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\n‚úÖ Saved raw data to: {raw_file}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
        analysis_data = {
            'timestamp': timestamp,
            'language_style': language_analysis,
            'dialog_patterns': dialog_analysis
        }
        
        analysis_file = f"data/natasha_style_analysis_{timestamp}.json"
        
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Saved analysis to: {analysis_file}")
        
        return raw_file, analysis_file


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("\n" + "="*70)
    print("NATASHA VOLKOSH STYLE EXTRACTION")
    print("="*70 + "\n")
    
    extractor = NatashaStyleExtractor()
    
    try:
        await extractor.client.connect()
        
        if not await extractor.client.is_user_authorized():
            logger.error("‚ùå Not authorized!")
            return
        
        logger.info("‚úÖ Connected to Telegram\n")
        
        # 1. –ù–∞—Ö–æ–¥–∏–º broadcast –∫–∞–Ω–∞–ª—ã
        logger.info("STEP 1: Finding broadcast channels...")
        broadcast_keywords = [
            'relove rituals',
            '–ø—É—Ç—å –≥–µ—Ä–æ—è',
            '–±–æ–ª—å—à–∞—è –∏–≥—Ä–∞',
            'relove people',
            '–ø—Ä–æ—à–ª—ã–µ –∂–∏–∑–Ω–∏'
        ]
        
        broadcast_channels = await extractor.find_channels_by_name(broadcast_keywords)
        
        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å—Ç—ã
        logger.info("\nSTEP 2: Extracting posts from channels...")
        for channel in broadcast_channels:
            await extractor.extract_posts_from_channel(
                channel['entity'],
                channel['name'],
                limit=100
            )
        
        # 3. –ù–∞—Ö–æ–¥–∏–º —á–∞—Ç—ã
        logger.info("\nSTEP 3: Finding discussion chats...")
        chat_keywords = [
            '—á–∞—Ç relove',
            '–±–æ–ª—å—à–∞—è –∏–≥—Ä–∞',
            'people chat',
            '–ø—É—Ç—å –≥–µ—Ä–æ—è chat',
            '–ø—Ä–æ—à–ª—ã–µ –∂–∏–∑–Ω–∏'
        ]
        
        discussion_chats = await extractor.find_channels_by_name(chat_keywords)
        
        # 4. –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∏–∞–ª–æ–≥–∏
        logger.info("\nSTEP 4: Extracting dialogs from chats...")
        for chat in discussion_chats:
            await extractor.extract_dialogs_from_chat(
                chat['entity'],
                chat['name'],
                limit=500
            )
        
        # 5. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∏–ª—å
        logger.info("\nSTEP 5: Analyzing language style...")
        language_analysis = await extractor.analyze_language_style()
        
        # 6. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∏–∞–ª–æ–≥–∏
        logger.info("\nSTEP 6: Analyzing dialog patterns...")
        dialog_analysis = await extractor.analyze_dialog_patterns()
        
        # 7. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info("\nSTEP 7: Saving results...")
        raw_file, analysis_file = await extractor.save_results(
            language_analysis,
            dialog_analysis
        )
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.info(f"\n{'='*70}")
        logger.info("STATISTICS")
        logger.info(f"{'='*70}")
        logger.info(f"Broadcast channels processed: {len(broadcast_channels)}")
        logger.info(f"Discussion chats processed: {len(discussion_chats)}")
        logger.info(f"Posts extracted: {len(extractor.natasha_posts)}")
        logger.info(f"Dialog contexts extracted: {len(extractor.natasha_dialogs)}")
        logger.info(f"\nFiles created:")
        logger.info(f"  - {raw_file}")
        logger.info(f"  - {analysis_file}")
        logger.info(f"{'='*70}\n")
        
        logger.info("‚úÖ Extraction complete!")
        logger.info("\nüí° Next step: Integrate into prompts")
        logger.info("   python scripts/analysis/integrate_natasha_style.py")
        
    except Exception as e:
        logger.error(f"‚ùå Fatal error: {e}", exc_info=True)
    
    finally:
        await extractor.client.disconnect()
        logger.info("\nüëã Disconnected from Telegram")


if __name__ == "__main__":
    asyncio.run(main())
