"""
–ê–Ω–∞–ª–∏–∑ —Å—Ç–∏–ª—è –ù–∞—Ç–∞—à–∏ –í–æ–ª–∫–æ—à —á–µ—Ä–µ–∑ LLM –¥–ª—è –∫—Ä–∏—Å—Ç–∞–ª–ª–∏–∑–∞—Ü–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.

–ó–∞–¥–∞—á–∏:
1. –ê–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤ —Å reply-—Ü–µ–ø–æ—á–∫–∞–º–∏
2. –í—ã—è–≤–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ–±—â–µ–Ω–∏—è
3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞—Ñ–æ—Ä –∏ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
4. –ö—Ä–∏—Å—Ç–∞–ª–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∏–ª—è –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤
"""
import asyncio
import json
import sys
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from relove_bot.config import settings
from relove_bot.services.llm_service import LLMService
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/natasha_style_analysis.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


STYLE_ANALYSIS_PROMPT = """
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Å—Ç–∏–ª—è –æ–±—â–µ–Ω–∏—è –∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∏–∞–ª–æ–≥–∏ –ù–∞—Ç–∞—à–∏ –í–æ–ª–∫–æ—à –∏ –≤—ã—è–≤–∏:

1. **–Ø–ó–´–ö–û–í–´–ï –ü–ê–¢–¢–ï–†–ù–´**:
   - –¢–∏–ø–∏—á–Ω—ã–µ —Ñ—Ä–∞–∑—ã –∏ –æ–±–æ—Ä–æ—Ç—ã
   - –î–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–∫–æ—Ä–æ—Ç–∫–∏–µ/–¥–ª–∏–Ω–Ω—ã–µ)
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤ vs —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π
   - –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ–∫—Ä–∞—Å–∫–∞ (–¥–∏—Ä–µ–∫—Ç–∏–≤–Ω–∞—è, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∞—è, –ø—Ä–æ–≤–æ–∫–∞—Ç–∏–≤–Ω–∞—è)

2. **–ú–ï–¢–ê–§–û–†–´ –ò –ö–û–ù–¶–ï–ü–¢–´**:
   - –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –º–µ—Ç–∞—Ñ–æ—Ä—ã (—Å–≤–µ—Ç/—Ç—å–º–∞, —Å–º–µ—Ä—Ç—å/–∂–∏–∑–Ω—å, –≤–æ–π–Ω–∞/–º–∏—Ä)
   - –î—É—Ö–æ–≤–Ω—ã–µ/—ç–∑–æ—Ç–µ—Ä–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
   - –ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
   - –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è

3. **–°–¢–†–£–ö–¢–£–†–ê –î–ò–ê–õ–û–ì–ê**:
   - –ö–∞–∫ –Ω–∞—á–∏–Ω–∞–µ—Ç –¥–∏–∞–ª–æ–≥
   - –ö–∞–∫ –∑–∞–¥–∞—ë—Ç –≤–æ–ø—Ä–æ—Å—ã (–ø—Ä—è–º—ã–µ/–∫–æ—Å–≤–µ–Ω–Ω—ã–µ)
   - –ö–∞–∫ –¥–∞—ë—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å
   - –ö–∞–∫ –∑–∞–≤–µ—Ä—à–∞–µ—Ç –¥–∏–∞–ª–æ–≥

4. **–ü–†–û–í–û–ö–ê–¢–ò–í–ù–´–ï –¢–ï–•–ù–ò–ö–ò**:
   - –ü—Ä—è–º–∞—è –∫–æ–Ω—Ñ—Ä–æ–Ω—Ç–∞—Ü–∏—è
   - –ü–µ—Ä–µ–≤–æ—Ä–æ—Ç –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã
   - –ê–±—Å—É—Ä–¥–Ω—ã–µ –º–µ—Ç–∞—Ñ–æ—Ä—ã
   - –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Å–∫–æ–º—Ñ–æ—Ä—Ç–∞

5. **–≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–ê–Ø –î–ò–ù–ê–ú–ò–ö–ê**:
   - –£–¥–∞—Ä ‚Üí –ü–æ–¥–¥–µ—Ä–∂–∫–∞
   - –ñ—ë—Å—Ç–∫–æ—Å—Ç—å ‚Üí –¢–µ–ø–ª–æ
   - –í—ã–∑–æ–≤ ‚Üí –ü—Ä–∏–Ω—è—Ç–∏–µ

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–µ –¥–∏–∞–ª–æ–≥–∏ –∏ –≤—ã–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑.

–î–ò–ê–õ–û–ì–ò:
{dialogs}

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):
{{
  "language_patterns": {{
    "typical_phrases": ["—Å–ø–∏—Å–æ–∫ —Ç–∏–ø–∏—á–Ω—ã—Ö —Ñ—Ä–∞–∑"],
    "sentence_structure": "–æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π",
    "question_style": "—Å—Ç–∏–ª—å –≤–æ–ø—Ä–æ—Å–æ–≤",
    "emotional_tone": "—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω"
  }},
  "metaphors_concepts": {{
    "recurring_metaphors": ["–ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –º–µ—Ç–∞—Ñ–æ—Ä—ã"],
    "spiritual_concepts": ["–¥—É—Ö–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã"],
    "psychological_terms": ["–ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã"],
    "unique_expressions": ["—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è"]
  }},
  "dialog_structure": {{
    "opening": "–∫–∞–∫ –Ω–∞—á–∏–Ω–∞–µ—Ç",
    "questioning": "–∫–∞–∫ –∑–∞–¥–∞—ë—Ç –≤–æ–ø—Ä–æ—Å—ã",
    "feedback": "–∫–∞–∫ –¥–∞—ë—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å",
    "closing": "–∫–∞–∫ –∑–∞–≤–µ—Ä—à–∞–µ—Ç"
  }},
  "provocative_techniques": {{
    "confrontation": ["–ø—Ä–∏–º–µ—Ä—ã –∫–æ–Ω—Ñ—Ä–æ–Ω—Ç–∞—Ü–∏–∏"],
    "perspective_flip": ["–ø—Ä–∏–º–µ—Ä—ã –ø–µ—Ä–µ–≤–æ—Ä–æ—Ç–∞ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã"],
    "absurd_metaphors": ["–∞–±—Å—É—Ä–¥–Ω—ã–µ –º–µ—Ç–∞—Ñ–æ—Ä—ã"],
    "discomfort_creation": ["—Å–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Å–∫–æ–º—Ñ–æ—Ä—Ç–∞"]
  }},
  "emotional_dynamics": {{
    "hit_support": ["–ø—Ä–∏–º–µ—Ä—ã —É–¥–∞—Ä‚Üí–ø–æ–¥–¥–µ—Ä–∂–∫–∞"],
    "harshness_warmth": ["–ø—Ä–∏–º–µ—Ä—ã –∂—ë—Å—Ç–∫–æ—Å—Ç—å‚Üí—Ç–µ–ø–ª–æ"],
    "challenge_acceptance": ["–ø—Ä–∏–º–µ—Ä—ã –≤—ã–∑–æ–≤‚Üí–ø—Ä–∏–Ω—è—Ç–∏–µ"]
  }},
  "key_insights": ["–∫–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã –æ —Å—Ç–∏–ª–µ"]
}}
"""


PATTERN_EXTRACTION_PROMPT = """
–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∏–ª—è –ù–∞—Ç–∞—à–∏, –∏–∑–≤–ª–µ–∫–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ –ø—Ä–æ–º–ø—Ç—ã –±–æ—Ç–∞.

–ê–ù–ê–õ–ò–ó –°–¢–ò–õ–Ø:
{style_analysis}

–ó–ê–î–ê–ß–ê:
–°–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –ø—Ä–æ–º–ø—Ç–∞—Ö:

1. **–§–†–ê–ó–´-–¢–†–ò–ì–ì–ï–†–´** ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –ø—Ä–æ–≤–æ–∫–∞—Ü–∏–∏
2. **–í–û–ü–†–û–°–´-–õ–û–í–£–®–ö–ò** ‚Äî –≤–æ–ø—Ä–æ—Å—ã, –≤–µ–¥—É—â–∏–µ –∫ –æ—Å–æ–∑–Ω–∞–Ω–∏—é
3. **–ú–ï–¢–ê–§–û–†–´-–ò–ù–°–¢–†–£–ú–ï–ù–¢–´** ‚Äî –º–µ—Ç–∞—Ñ–æ—Ä—ã –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
4. **–¢–ï–•–ù–ò–ö–ò –ü–ï–†–ï–í–û–†–û–¢–ê** ‚Äî —Å–ø–æ—Å–æ–±—ã –ø–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç—å –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—É
5. **–ü–û–î–î–ï–†–ñ–ò–í–ê–Æ–©–ò–ï –§–†–ê–ó–´** ‚Äî —Ñ—Ä–∞–∑—ã –ø–æ—Å–ª–µ –∂—ë—Å—Ç–∫–æ–≥–æ –≤—Å–∫—Ä—ã—Ç–∏—è

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):
{{
  "trigger_phrases": [
    {{"phrase": "—Ñ—Ä–∞–∑–∞", "context": "–∫–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å", "example": "–ø—Ä–∏–º–µ—Ä –∏–∑ –¥–∏–∞–ª–æ–≥–∞"}}
  ],
  "trap_questions": [
    {{"question": "–≤–æ–ø—Ä–æ—Å", "purpose": "—Ü–µ–ª—å –≤–æ–ø—Ä–æ—Å–∞", "example": "–ø—Ä–∏–º–µ—Ä"}}
  ],
  "metaphor_tools": [
    {{"metaphor": "–º–µ—Ç–∞—Ñ–æ—Ä–∞", "meaning": "–∑–Ω–∞—á–µ–Ω–∏–µ", "usage": "–∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å"}}
  ],
  "flip_techniques": [
    {{"technique": "—Ç–µ—Ö–Ω–∏–∫–∞", "description": "–æ–ø–∏—Å–∞–Ω–∏–µ", "example": "–ø—Ä–∏–º–µ—Ä"}}
  ],
  "support_phrases": [
    {{"phrase": "—Ñ—Ä–∞–∑–∞", "timing": "–∫–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å", "example": "–ø—Ä–∏–º–µ—Ä"}}
  ]
}}
"""


class NatashaStyleAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å—Ç–∏–ª—è –ù–∞—Ç–∞—à–∏ —á–µ—Ä–µ–∑ LLM."""
    
    def __init__(self):
        self.llm = LLMService()
    
    def load_dialogs(self, json_path: str) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∏–∞–ª–æ–≥–∏ –∏–∑ JSON."""
        logger.info(f"Loading dialogs from {json_path}")
        
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–∏–∞–ª–æ–≥–∏ —Å reply-—Ü–µ–ø–æ—á–∫–∞–º–∏
        dialogs_with_replies = [
            d for d in data.get('dialogs', [])
            if d.get('has_reply_chain', False)
        ]
        
        logger.info(f"Loaded {len(dialogs_with_replies)} dialogs with reply chains")
        return dialogs_with_replies
    
    def format_dialog_for_analysis(self, dialog: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."""
        lines = [
            f"=== –î–ò–ê–õ–û–ì: {dialog['chat']} ({dialog['date']}) ==="
        ]
        
        for msg in dialog['context']:
            role = "üî• –ù–ê–¢–ê–®–ê" if msg['is_natasha'] else f"üë§ {msg['sender_name']}"
            lines.append(f"{role}: {msg['text']}")
        
        lines.append("")
        return "\n".join(lines)
    
    async def analyze_style(
        self,
        dialogs: List[Dict[str, Any]],
        sample_size: int = 50
    ) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç–∏–ª—å —á–µ—Ä–µ–∑ LLM."""
        logger.info(f"Analyzing style from {sample_size} dialogs...")
        
        # –ë–µ—Ä—ë–º sample –¥–∏–∞–ª–æ–≥–æ–≤
        sample_dialogs = dialogs[:sample_size]
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        formatted_dialogs = "\n\n".join([
            self.format_dialog_for_analysis(d)
            for d in sample_dialogs
        ])
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ LLM
        prompt = STYLE_ANALYSIS_PROMPT.format(dialogs=formatted_dialogs)
        
        logger.info("Sending to LLM for style analysis...")
        response = await self.llm.generate_text(
            prompt=prompt,
            max_tokens=4000,
            temperature=0.7
        )
        
        # –ü–∞—Ä—Å–∏–º JSON
        try:
            style_analysis = json.loads(response)
            logger.info("‚úÖ Style analysis completed")
            return style_analysis
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON: {e}")
            logger.error(f"Response: {response}")
            return {}
    
    async def extract_patterns(
        self,
        style_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤."""
        logger.info("Extracting patterns for prompts...")
        
        prompt = PATTERN_EXTRACTION_PROMPT.format(
            style_analysis=json.dumps(style_analysis, ensure_ascii=False, indent=2)
        )
        
        logger.info("Sending to LLM for pattern extraction...")
        response = await self.llm.generate_text(
            prompt=prompt,
            max_tokens=4000,
            temperature=0.7
        )
        
        # –ü–∞—Ä—Å–∏–º JSON
        try:
            patterns = json.loads(response)
            logger.info("‚úÖ Pattern extraction completed")
            return patterns
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON: {e}")
            logger.error(f"Response: {response}")
            return {}
    
    def save_results(
        self,
        style_analysis: Dict[str, Any],
        patterns: Dict[str, Any]
    ) -> tuple[str, str]:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ —Å—Ç–∏–ª—è
        style_file = f"data/natasha_style_analysis_{timestamp}.json"
        with open(style_file, 'w', encoding='utf-8') as f:
            json.dump(style_analysis, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Saved style analysis to: {style_file}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        patterns_file = f"data/natasha_patterns_{timestamp}.json"
        with open(patterns_file, 'w', encoding='utf-8') as f:
            json.dump(patterns, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Saved patterns to: {patterns_file}")
        
        # –°–æ–∑–¥–∞—ë–º markdown-–æ—Ç—á—ë—Ç
        report_file = f"data/natasha_style_report_{timestamp}.md"
        self._create_markdown_report(
            style_analysis,
            patterns,
            report_file
        )
        
        logger.info(f"‚úÖ Saved report to: {report_file}")
        
        return style_file, patterns_file, report_file
    
    def _create_markdown_report(
        self,
        style_analysis: Dict[str, Any],
        patterns: Dict[str, Any],
        output_file: str
    ):
        """–°–æ–∑–¥–∞—ë—Ç markdown-–æ—Ç—á—ë—Ç."""
        lines = [
            "# –ê–Ω–∞–ª–∏–∑ —Å—Ç–∏–ª—è –ù–∞—Ç–∞—à–∏ –í–æ–ª–∫–æ—à",
            "",
            "## 1. –Ø–∑—ã–∫–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã",
            ""
        ]
        
        # –Ø–∑—ã–∫–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        lang_patterns = style_analysis.get('language_patterns', {})
        if lang_patterns:
            lines.append("### –¢–∏–ø–∏—á–Ω—ã–µ —Ñ—Ä–∞–∑—ã:")
            for phrase in lang_patterns.get('typical_phrases', []):
                lines.append(f"- {phrase}")
            lines.append("")
            
            lines.append(f"**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π**: {lang_patterns.get('sentence_structure', 'N/A')}")
            lines.append(f"**–°—Ç–∏–ª—å –≤–æ–ø—Ä–æ—Å–æ–≤**: {lang_patterns.get('question_style', 'N/A')}")
            lines.append(f"**–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω**: {lang_patterns.get('emotional_tone', 'N/A')}")
            lines.append("")
        
        # –ú–µ—Ç–∞—Ñ–æ—Ä—ã –∏ –∫–æ–Ω—Ü–µ–ø—Ç—ã
        lines.append("## 2. –ú–µ—Ç–∞—Ñ–æ—Ä—ã –∏ –∫–æ–Ω—Ü–µ–ø—Ç—ã")
        lines.append("")
        
        metaphors = style_analysis.get('metaphors_concepts', {})
        if metaphors:
            lines.append("### –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –º–µ—Ç–∞—Ñ–æ—Ä—ã:")
            for m in metaphors.get('recurring_metaphors', []):
                lines.append(f"- {m}")
            lines.append("")
            
            lines.append("### –î—É—Ö–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã:")
            for c in metaphors.get('spiritual_concepts', []):
                lines.append(f"- {c}")
            lines.append("")
        
        # –ü—Ä–æ–≤–æ–∫–∞—Ç–∏–≤–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏
        lines.append("## 3. –ü—Ä–æ–≤–æ–∫–∞—Ç–∏–≤–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏")
        lines.append("")
        
        techniques = style_analysis.get('provocative_techniques', {})
        if techniques:
            lines.append("### –ö–æ–Ω—Ñ—Ä–æ–Ω—Ç–∞—Ü–∏—è:")
            for t in techniques.get('confrontation', []):
                lines.append(f"- {t}")
            lines.append("")
            
            lines.append("### –ü–µ—Ä–µ–≤–æ—Ä–æ—Ç –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã:")
            for t in techniques.get('perspective_flip', []):
                lines.append(f"- {t}")
            lines.append("")
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤
        lines.append("## 4. –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ –ø—Ä–æ–º–ø—Ç—ã")
        lines.append("")
        
        # –§—Ä–∞–∑—ã-—Ç—Ä–∏–≥–≥–µ—Ä—ã
        lines.append("### –§—Ä–∞–∑—ã-—Ç—Ä–∏–≥–≥–µ—Ä—ã:")
        for trigger in patterns.get('trigger_phrases', []):
            lines.append(f"- **{trigger.get('phrase')}**")
            lines.append(f"  - –ö–æ–Ω—Ç–µ–∫—Å—Ç: {trigger.get('context')}")
            lines.append(f"  - –ü—Ä–∏–º–µ—Ä: {trigger.get('example')}")
            lines.append("")
        
        # –í–æ–ø—Ä–æ—Å—ã-–ª–æ–≤—É—à–∫–∏
        lines.append("### –í–æ–ø—Ä–æ—Å—ã-–ª–æ–≤—É—à–∫–∏:")
        for question in patterns.get('trap_questions', []):
            lines.append(f"- **{question.get('question')}**")
            lines.append(f"  - –¶–µ–ª—å: {question.get('purpose')}")
            lines.append(f"  - –ü—Ä–∏–º–µ—Ä: {question.get('example')}")
            lines.append("")
        
        # –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã
        lines.append("## 5. –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã")
        lines.append("")
        for insight in style_analysis.get('key_insights', []):
            lines.append(f"- {insight}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("\n".join(lines))


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Analyze Natasha's communication style"
    )
    parser.add_argument(
        '--input',
        type=str,
        default='data/natasha_full_content_20251122_090907.json',
        help='Input JSON file with dialogs'
    )
    parser.add_argument(
        '--sample-size',
        type=int,
        default=50,
        help='Number of dialogs to analyze (default: 50)'
    )
    
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("NATASHA STYLE ANALYSIS")
    print("="*70 + "\n")
    
    analyzer = NatashaStyleAnalyzer()
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∏–∞–ª–æ–≥–∏
        dialogs = analyzer.load_dialogs(args.input)
        
        if not dialogs:
            logger.error("No dialogs with reply chains found!")
            return
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∏–ª—å
        style_analysis = await analyzer.analyze_style(
            dialogs,
            sample_size=args.sample_size
        )
        
        if not style_analysis:
            logger.error("Style analysis failed!")
            return
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        patterns = await analyzer.extract_patterns(style_analysis)
        
        if not patterns:
            logger.error("Pattern extraction failed!")
            return
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        style_file, patterns_file, report_file = analyzer.save_results(
            style_analysis,
            patterns
        )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.info(f"\n{'='*70}")
        logger.info("RESULTS")
        logger.info(f"{'='*70}")
        logger.info(f"Analyzed dialogs: {args.sample_size}")
        logger.info(f"Style analysis: {style_file}")
        logger.info(f"Patterns: {patterns_file}")
        logger.info(f"Report: {report_file}")
        logger.info(f"{'='*70}\n")
        
    except Exception as e:
        logger.error(f"‚ùå Fatal error: {e}", exc_info=True)


if __name__ == "__main__":
    asyncio.run(main())
