# üÜì –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã fine-tuning

**–î–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤!** –í–æ—Ç –≥–¥–µ –º–æ–∂–Ω–æ –∑–∞—Ñ–∞–π–Ω—Ç—é–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –±–µ—Å–ø–ª–∞—Ç–Ω–æ.

---

## 1Ô∏è‚É£ Hugging Face (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø)

### ‚úÖ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞

- ‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é –±–µ—Å–ø–ª–∞—Ç–Ω–æ
- ‚úÖ –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–æ—Ü–µ—Å—Å
- ‚úÖ –ë–æ–ª—å—à–æ–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–æ
- ‚úÖ –ì–æ—Ç–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
- ‚úÖ –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π —Ö–æ—Å—Ç–∏–Ω–≥

### üöÄ –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

```bash
# 1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install transformers torch peft datasets

# 2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç
# data/natasha_finetuning_20251125_153356.jsonl

# 3. –ó–∞–ø—É—Å—Ç–∏—Ç—å fine-tuning
python -c "
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import load_dataset

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
model_name = 'mistralai/Mistral-7B-Instruct-v0.1'
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
dataset = load_dataset('json', data_files='data/natasha_finetuning_20251125_153356.jsonl')

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir='./natasha-model',
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=10,
    save_total_limit=2,
)

# –û–±—É—á–∞–µ–º
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset['train'],
)
trainer.train()
"
```

### üìä –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä | –ö–∞—á–µ—Å—Ç–≤–æ | –°–∫–æ—Ä–æ—Å—Ç—å |
|--------|--------|----------|----------|
| Mistral-7B | 7B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Llama-2-7B | 7B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Phi-2 | 2.7B | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Gemma-7B | 7B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |

---

## 2Ô∏è‚É£ Google Colab (–ë–ï–°–ü–õ–ê–¢–ù–´–ô GPU)

### ‚úÖ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞

- ‚úÖ –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π GPU (T4)
- ‚úÖ –ù–µ –Ω—É–∂–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –Ω–∏—á–µ–≥–æ
- ‚úÖ –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- ‚úÖ –ì–æ—Ç–æ–≤–∞—è —Å—Ä–µ–¥–∞

### üöÄ –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

1. –ü–µ—Ä–µ–π—Ç–∏ –Ω–∞ https://colab.research.google.com/
2. –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π notebook
3. –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç
4. –ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–¥:

```python
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
!pip install transformers torch peft datasets

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç
from google.colab import files
uploaded = files.upload()

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import load_dataset

model_name = 'mistralai/Mistral-7B-Instruct-v0.1'
model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')
tokenizer = AutoTokenizer.from_pretrained(model_name)

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç
dataset = load_dataset('json', data_files='natasha_finetuning_20251125_153356.jsonl')

# –û–±—É—á–∏—Ç—å
training_args = TrainingArguments(
    output_dir='./natasha-model',
    num_train_epochs=3,
    per_device_train_batch_size=4,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset['train'],
)
trainer.train()

# –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å
model.save_pretrained('./natasha-model')
```

---

## 3Ô∏è‚É£ Replicate (–ë–ï–°–ü–õ–ê–¢–ù–´–ô TRIAL)

### ‚úÖ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞

- ‚úÖ –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π trial ($5)
- ‚úÖ –ü—Ä–æ—Å—Ç–æ–π API
- ‚úÖ –ì–æ—Ç–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
- ‚úÖ –•–æ—Å—Ç–∏–Ω–≥ –≤–∫–ª—é—á–µ–Ω

### üöÄ –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

1. –ü–µ—Ä–µ–π—Ç–∏ –Ω–∞ https://replicate.com/
2. –°–æ–∑–¥–∞—Ç—å –∞–∫–∫–∞—É–Ω—Ç
3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å trial –∫—Ä–µ–¥–∏—Ç—ã
4. –ó–∞–ø—É—Å—Ç–∏—Ç—å fine-tuning

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å CLI
pip install replicate

# –ó–∞–ø—É—Å—Ç–∏—Ç—å fine-tuning
replicate.run(
    "mistralai/mistral-7b-instruct-v0.1:fine-tune",
    input={
        "train_data": open("data/natasha_finetuning_20251125_153356.jsonl"),
        "num_train_epochs": 3,
    }
)
```

---

## 4Ô∏è‚É£ Kaggle Notebooks (–ë–ï–°–ü–õ–ê–¢–ù–´–ô GPU)

### ‚úÖ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞

- ‚úÖ –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π GPU (P100)
- ‚úÖ 30 —á–∞—Å–æ–≤ –≤ –Ω–µ–¥–µ–ª—é
- ‚úÖ –ì–æ—Ç–æ–≤–∞—è —Å—Ä–µ–¥–∞
- ‚úÖ –ë–æ–ª—å—à–æ–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–æ

### üöÄ –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

1. –ü–µ—Ä–µ–π—Ç–∏ –Ω–∞ https://www.kaggle.com/
2. –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π notebook
3. –í–∫–ª—é—á–∏—Ç—å GPU (Settings ‚Üí Accelerator ‚Üí GPU)
4. –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç
5. –ó–∞–ø—É—Å—Ç–∏—Ç—å fine-tuning

---

## 5Ô∏è‚É£ Lambda Labs (–ë–ï–°–ü–õ–ê–¢–ù–´–ô TRIAL)

### ‚úÖ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞

- ‚úÖ –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π trial ($10)
- ‚úÖ –ú–æ—â–Ω—ã–µ GPU (A100)
- ‚úÖ –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- ‚úÖ –ü—Ä–æ—Å—Ç–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

### üöÄ –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

1. –ü–µ—Ä–µ–π—Ç–∏ –Ω–∞ https://lambdalabs.com/
2. –°–æ–∑–¥–∞—Ç—å –∞–∫–∫–∞—É–Ω—Ç
3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å trial –∫—Ä–µ–¥–∏—Ç—ã
4. –ó–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω—Å—Ç–∞–Ω—Å —Å GPU
5. –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –∏ –æ–±—É—á–∏—Ç—å

---

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ

| –í–∞—Ä–∏–∞–Ω—Ç | –°—Ç–æ–∏–º–æ—Å—Ç—å | GPU | –°–∫–æ—Ä–æ—Å—Ç—å | –°–ª–æ–∂–Ω–æ—Å—Ç—å |
|---------|-----------|-----|----------|-----------|
| **Hugging Face** | üÜì –ë–µ—Å–ø–ª–∞—Ç–Ω–æ | –ù–µ—Ç | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Google Colab** | üÜì –ë–µ—Å–ø–ª–∞—Ç–Ω–æ | T4 | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **Replicate** | üÜì $5 trial | –î–∞ | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê |
| **Kaggle** | üÜì –ë–µ—Å–ø–ª–∞—Ç–Ω–æ | P100 | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **Lambda Labs** | üÜì $10 trial | A100 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |

---

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è

### –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞: **Google Colab**

- –ë–µ—Å–ø–ª–∞—Ç–Ω–æ
- –ë—ã—Å—Ç—Ä–æ
- –ü—Ä–æ—Å—Ç–æ
- –ì–æ—Ç–æ–≤–∞—è —Å—Ä–µ–¥–∞

### –î–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞: **Hugging Face + Mistral-7B**

- –ë–µ—Å–ø–ª–∞—Ç–Ω–æ
- –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
- –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ
- –ë–æ–ª—å—à–æ–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–æ

### –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏: **Lambda Labs**

- –ú–æ—â–Ω—ã–µ GPU (A100)
- –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π trial ($10)

---

## üìù –î–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–∞–π–ª:
```
data/natasha_finetuning_20251125_153356.jsonl
```

–í—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç —ç—Ç–æ—Ç —Ñ–æ—Ä–º–∞—Ç!

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç —Å Google Colab

1. –ü–µ—Ä–µ–π—Ç–∏ –Ω–∞ https://colab.research.google.com/
2. –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π notebook
3. –°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥ –≤—ã—à–µ
4. –ó–∞–ø—É—Å—Ç–∏—Ç—å
5. –ü–æ–ª—É—á–∏—Ç—å fine-tuned –º–æ–¥–µ–ª—å

**–í—Ä–µ–º—è**: ~1-2 —á–∞—Å–∞  
**–°—Ç–æ–∏–º–æ—Å—Ç—å**: üÜì –ë–µ—Å–ø–ª–∞—Ç–Ω–æ  
**–ö–∞—á–µ—Å—Ç–≤–æ**: ‚≠ê‚≠ê‚≠ê‚≠ê

---

**–í—ã–±–∏—Ä–∞–π—Ç–µ –≤–∞—Ä–∏–∞–Ω—Ç –∏ –Ω–∞—á–∏–Ω–∞–π—Ç–µ!** üöÄ
